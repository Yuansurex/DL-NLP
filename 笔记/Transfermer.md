报告嘉宾：邱锡鹏 (复旦大学) 

**报告题目：A Tutorial of Transformers** 

报告摘要： 本报告主要介绍Transformer模型以及变体，主要涵盖两部分内容：
1）Transformer模型介绍：介绍自注意力模型以及Transformer的基本架构并分析模型优缺点；
2）Transformer模型的改进，通过针对性的改进来进一步提高Transformer模型的效率、泛化性。 

综述文章: https://arxiv.org/abs/2106.04554

***Attention Is All You Need***

**自注意**![image-20211018124706689](C:\Users\37448\AppData\Roaming\Typora\typora-user-images\image-20211018124706689.png)

**QKV模式**

![image-20211018134328904](C:\Users\37448\AppData\Roaming\Typora\typora-user-images\image-20211018134328904.png)

**Multi-head Self-Attention**

![image-20211018134623837](C:\Users\37448\AppData\Roaming\Typora\typora-user-images\image-20211018134623837.png)

![image-20211018134716434](C:\Users\37448\AppData\Roaming\Typora\typora-user-images\image-20211018134716434.png)

