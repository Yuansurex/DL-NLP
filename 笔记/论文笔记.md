## Attention Is All You Need

#### **为什么要做这个研究？理论走向和目前缺陷**

当前语言模型依靠序列建模，顺序输入，计算无法并行。

#### **怎么做这个研究？方法，尤其是不同之处**

提出了`Transformer`这个新的模型结构，不使用序列对齐的RNN或卷积，完全基于注意力机制，绘制了输入输出的全局依赖。



训练：

优化：Adam

正规化：
（1) dropout :每个子层输出···
（2) Label Smoothing：减少困惑，提高准确性和BLEU分数

#### **发现了什么？总结结果，补充和理论的关系**

1. 成本更少，效果更好，达到翻译质量新水平
2. 可以并行计算，提升效率

#### 作者计划未来做什么：

- [ ] 提高长序列的计算性能，将自注意限制到输入序列以各自输出位置为中心的一个r size临近，增大最大路径到O(n/r)。

#### 阅读困难的地方

1. 什么是自回归？
2. 什么是多头注意力？
3. 模型看不懂?



